environment_name: 'MVC'  #', metavar='ENV_CLASS', type=str, default='MVC', help='Class to use for the environment. Must be in the \'environment\' module')
agent: 'Agent' # metavar='AGENT_CLASS', default='Agent', type=str, help='Class to use for the agent. Must be in the \'agent\' module.')
model: 'S2V_QN_1' #, type=str, default='S2V_QN_1', help='model name') # ['S2V_QN_1', 'S2V_QN_2', 'GCN_QN_1', 'LINE_QN', 'W2V_QN']
ngames: 1  # type=int, metavar='n', default=1, help='number of games to simulate')
max_episodes: 1  # type=int, metavar='n', default='1000', help='max number of iterations per game')
nbr_epoch: 10 # type=int, metavar='nepoch', default=50, help="number of epochs")
lr: 0.001 # type=float, default=1e-4, help="learning rate")
bs: 32 # type=int, default=32, help="minibatch size for training")
n_step: 3 # type=int, default=3, help="n step in RL")
batch: None  # type=int, metavar='nagent', default=None, help='batch run several agent at the same time')
verbose: True # action='store_true', default=True, help='Display cumulative results at each step')
task: 'colge'

# epsilon greedy
init_epsilon: 0.99
final_epsilon: 0.01
epislon_decay_steps: 10000


method: 'rl'  # ['rl', 'ada_greedy', 'lazy_adaptive_greedy']

use_state_abs: True

#graph_type: 'erdos_renyi' # , metavar='GRAPH', default='erdos_renyi', help ='Type of graph to optimize')
#graph_nbr: 8 # , type=int, default='1', help='number of differente graph to generate for the training sample')
#node: 20 # type=int, metavar='nnode', default=20, help="number of node in generated graphs")
#p: 0.14  #, default=0.14, help="p, parameter in graph degree distribution")
#m: 4  #, default=4, help="m, parameter in graph degree distribution")
