environment_name: 'IM'  #'type=str 
agent: 'Agent' # metavar='AGENT_CLASS', default='Agent', type=str, help='Class to use for the agent. Must be in the \'agent\' module.')
model: 'S2V_QN_1' #type=str, default='S2V_QN_1', help='network model name') # ['S2V_QN_1', 'S2V_QN_2', 'GCN_QN_1', 'LINE_QN', 'W2V_QN']
ngames: 1  # type=int, metavar='n', default=1, help='number of games to simulate')
max_episodes: 1  # type=int, metavar='n', default='1000', help='max number of iterations per game')
max_global_t: 2000
nbr_epoch: 10000 # type=int, metavar='nepoch', default=10000, help="maximal number of epochs")
lr: 0.001 # type=float, default=1e-4, help="learning rate")
bs: 32 # type=int, default=32, help="minibatch size for training")
n_step: 3 # type=int, default=3, help="n step in RL")
batch: None  # type=int, metavar='nagent', default=None, help='batch run several agent at the same time')
verbose: True #False # action='store_true', default=True, help='Display cumulative results at each step')
task: 'rl4im'
save_every: 20 # save model and do validation every few episodes, default 20
num_simul_train: 200
num_simul_test: 1000

# epsilon greedy
init_epsilon: 0.99
final_epsilon: 0.01
epsilon_decay_steps: 1000
#
#
method: 'rl'  # ['rl', 'ada_greedy', 'lazy_adaptive_greedy']
#
# these are used as ablation study
use_state_abs: True
reward_type: 3 #int {0, 1, 2, 3}

model_scheme: 'type1'  # ways to handle various graph size in graph embedding #{'normal': 'run graphs with same sizes', 'type1': 'padding', 'type2': 'for loop for graphs with various nodes'} #TODO
